#!/usr/bin/env python3
"""
åˆ†ææ‰€æœ‰è§’è‰²çš„BUFFç±»å‹ï¼Œæ‰¾å‡ºæ–‡æ¡£ä¸­æœªæ¶µç›–çš„BUFFç±»å‹
"""

import json
import re
from pathlib import Path
from collections import defaultdict

# æ–‡æ¡£ä¸­å·²è®°å½•çš„BUFFç±»å‹å…³é”®è¯
DOCUMENTED_BUFF_TYPES = {
    # åŸºç¡€å±æ€§
    "ATK_", "HP_", "DEF_", "CRIT_", "CRIT_DMG_", "PEN_",
    "ANOM_MAS", "ANOM_PROF", "ENER_REGEN_",
    # ä¼¤å®³åŠ æˆ
    "DMG_", "NORMAL_ATK_DMG_", "ENHANCED_SPECIAL_DMG_", 
    "CHAIN_ATK_DMG_", "ULTIMATE_ATK_DMG_",
    # å…ƒç´ ä¼¤å®³
    "PHYSICAL_DMG_", "FIRE_DMG_", "ICE_DMG_", "ELECTRIC_DMG_", "ETHER_DMG_",
    # Debuff
    "DEF_RED_", "RES_RED_", "DAMAGE_TAKEN_",
}

# æ–‡æ¡£ä¸­å·²è®°å½•çš„BUFFæœºåˆ¶å…³é”®è¯
DOCUMENTED_MECHANISMS = {
    # æ¡ä»¶è§¦å‘ç±»
    "æ”»å‡»å‘½ä¸­", "ç”Ÿå‘½å€¼", "æš´å‡»", "é—ªé¿", "å†²åˆº", "æ™®é€šæ”»å‡»", "ç‰¹æ®ŠæŠ€", 
    "è¿æºæŠ€", "ç»ˆç»“æŠ€", "å¼ºåŒ–ç‰¹æ®ŠæŠ€", "å¤±è¡¡", "åå°", "é˜Ÿä¼", "å…¨é˜Ÿ",
    # æ•ˆæœç±»å‹
    "ä¼¤å®³æå‡", "æ”»å‡»åŠ›æå‡", "æš´å‡»ç‡æå‡", "æš´å‡»ä¼¤å®³æå‡", 
    "é˜²å¾¡é™ä½", "æŠ—æ€§é™ä½", "æ˜“ä¼¤", "èƒ½é‡å›å¤", "å›èƒ½",
    "æŒç»­æ—¶é—´", "å åŠ ", "å±‚æ•°", "ä¸Šé™",
}

# æ–‡æ¡£ä¸­çš„BUFFç±»å‹åˆ†ç±»
DOCUMENTED_CATEGORIES = {
    "Buff": ["æ”»å‡»åŠ›æå‡", "æš´å‡»ç‡æå‡", "æš´å‡»ä¼¤å®³æå‡", "é˜²å¾¡é™ä½", "ä¼¤å®³æå‡"],
    "ConversionBuff": ["åŸºäºXæå‡Y"],
    "BuffTarget": ["è‡ªèº«", "é˜Ÿå‹", "æ•Œäºº", "é‚¦å¸ƒ"],
}


def extract_buff_mentions(text: str) -> list:
    """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰ç±»ä¼¼BUFFçš„æ•ˆæœæè¿°"""
    mentions = []
    
    # æå–æ‰€æœ‰ "Xæå‡/é™ä½Y%" æˆ– "Xæå‡/é™ä½Yç‚¹" çš„æ¨¡å¼
    pattern = r'([^\sï¼Œã€ï¼Œã€‚ï¼ï¼Ÿ]+(?:ç‡|å€¼|ä¼¤å®³|åŠ æˆ|æ•ˆç‡|ç²¾é€š|æŒæ§|å›å¤|æŒç»­|å±‚æ•°|æ—¶é—´){1,3}(?:æå‡|é™ä½|å¢åŠ |å‡å°‘)[^\sï¼Œã€ï¼Œã€‚ï¼ï¼Ÿ]{0,15}(?:%|ç‚¹|ç§’)?)'
    matches = re.findall(pattern, text)
    mentions.extend(matches)
    
    # æå–æ¡ä»¶è§¦å‘ç±»æ•ˆæœ
    condition_pattern = r'(?:å½“|å‘åŠ¨|æ”»å‡»|ä½¿ç”¨|å‘½ä¸­|å¤„äº)[^\sï¼Œã€ï¼Œã€‚ï¼ï¼Ÿ]{0,20}(?:æ—¶|å){1}'
    cond_matches = re.findall(condition_pattern, text)
    mentions.extend(cond_matches)
    
    # æå–ç›®æ ‡ç›¸å…³æ•ˆæœ
    target_pattern = r'(?:å…¨é˜Ÿ|é˜Ÿå‹|åå°|è‡ªèº«|æ•Œäºº|è‡ªèº«|è‡ªèº«){1}'
    target_matches = re.findall(target_pattern, text)
    mentions.extend(target_matches)
    
    return list(set(mentions))


def analyze_character(char_id: str, char_data: dict) -> dict:
    """åˆ†æå•ä¸ªè§’è‰²çš„BUFFç±»å‹"""
    result = {
        "id": char_id,
        "name": char_data.get("Name", ""),
        "undocumented_effects": [],
        "unique_keywords": [],
        "mechanism_types": [],
    }
    
    # åˆ†ææ ¸å¿ƒè¢«åŠ¨å’Œå¤©èµ‹
    passive_data = char_data.get("Passive", {}).get("Level", {})
    talent_data = char_data.get("Talent", {})
    potential_data = char_data.get("Potential", [])
    
    all_text = []
    
    # æ”¶é›†æ‰€æœ‰æè¿°æ–‡æœ¬
    for level_data in passive_data.values():
        names = level_data.get("Name", [])
        descs = level_data.get("Desc", [])
        if isinstance(names, list) and isinstance(descs, list):
            for name, desc in zip(names, descs):
                all_text.append(f"[æ ¸å¿ƒè¢«åŠ¨]{name}: {desc}")
                all_text.append(desc)
    
    for level, talent in talent_data.items():
        desc = talent.get("Desc", "")
        desc2 = talent.get("Desc2", "")
        all_text.append(f"[å¤©èµ‹{level}]{talent.get('Name', '')}: {desc}")
        all_text.append(desc)
        all_text.append(desc2)
    
    for pot in potential_data:
        all_text.append(f"[æ½œèƒ½]{pot.get('Name', '')}: {pot.get('Desc', '')}")
        all_text.append(pot.get("Desc", ""))
    
    # åˆ†æç‰¹æ®Šæ•ˆæœç±»å‹
    special_effects = defaultdict(list)
    
    for text in all_text:
        if not text or not isinstance(text, str):
            continue
            
        # æ£€æµ‹ç‰¹æ®ŠBUFFæœºåˆ¶
        # 1. èƒ½é‡ç›¸å…³
        if re.search(r'èƒ½é‡|å›èƒ½|å……èƒ½', text):
            special_effects["ENERGY_RELATED"].append(text[:50])
        
        # 2. å¤±è¡¡ç›¸å…³
        if re.search(r'å¤±è¡¡å€¼?|å¤±è¡¡çŠ¶æ€', text):
            special_effects["STUN_RELATED"].append(text[:50])
        
        # 3. å±æ€§å¼‚å¸¸
        if re.search(r'å±æ€§å¼‚å¸¸|å¼‚å¸¸ç§¯è“„|å¼‚å¸¸æŒæ§|å¼‚å¸¸ç²¾é€š', text):
            special_effects["ANOMALY_RELATED"].append(text[:50])
        
        # 4. æŠ—æ‰“æ–­
        if re.search(r'æŠ—æ‰“æ–­|æ— æ•Œ|éœ¸ä½“', text):
            special_effects["INTERRUPT_IMMUNE"].append(text[:50])
        
        # 5. æš´å‡»ç›¸å…³ï¼ˆè¶…å‡ºæ–‡æ¡£ï¼‰
        if re.search(r'æš´å‡»æš´å‡»|æš´å‡»åŠ æˆ', text):
            special_effects["CRIT_SPECIAL"].append(text[:50])
        
        # 6. è¿å‡»/è¿æ®µ
        if re.search(r'è¿å‡»|è¿æ®µ|è¿ç»­', text):
            special_effects["COMBO_RELATED"].append(text[:50])
        
        # 7. ç‰¹æ®Šè§¦å‘æ¡ä»¶
        if re.search(r'æé™é—ªé¿|å¿«é€Ÿæ”¯æ´|æ‹›æ¶æ”¯æ´', text):
            special_effects["SPECIAL_TRIGGER"].append(text[:50])
        
        # 8. å¤šå±‚/å å±‚
        if re.search(r'å åŠ |å±‚æ•°|ä¸Šé™', text):
            special_effects["STACK_MECHANIC"].append(text[:50])
        
        # 9. åœºä¸Š/åå°
        if re.search(r'åœºä¸Š|åå°|å½“å‰è§’è‰²', text):
            special_effects["POSITION_BASED"].append(text[:50])
        
        # 10. ç‰¹å®šå±æ€§/é˜µè¥
        if re.search(r'ç”µå±æ€§|ç«å±æ€§|å†°å±æ€§|ç‰©ç†|ä»¥å¤ª|ç‹¡å…”å±‹|ç»´å¤šåˆ©äºš|', text):
            special_effects["ATTRIBUTE_BASED"].append(text[:50])
        
        # 11. å¢ç›Š/å‡ç›ŠæŒç»­æ—¶é—´
        if re.search(r'æŒç»­.*ç§’|.*ç§’å†…', text):
            special_effects["DURATION_BASED"].append(text[:50])
        
        # 12. æš´å‡»å›èƒ½/æš´å‡»å›è¡€
        if re.search(r'æš´å‡»æ—¶.*å›å¤|æš´å‡».*å›', text):
            special_effects["CRIT_TRIGGER"].append(text[:50])
    
    result["special_effects"] = dict(special_effects)
    return result


def main():
    char_dir = Path.cwd() / r"assets\inventory_data\character"
    
    # æ”¶é›†æ‰€æœ‰ç‰¹æ®Šæ•ˆæœç±»å‹
    all_special_effects = defaultdict(set)
    all_keywords = defaultdict(set)
    
    print("æ­£åœ¨åˆ†æè§’è‰²æ•°æ®...\n")
    
    # åˆ†æå‰20ä¸ªè§’è‰²
    char_files = sorted(char_dir.glob("*.json"))[:20]
    
    for char_file in char_files:
        char_id = char_file.stem
        try:
            with open(char_file, 'r', encoding='utf-8') as f:
                char_data = json.load(f)
            
            result = analyze_character(char_id, char_data)
            
            # æ”¶é›†ç‰¹æ®Šæ•ˆæœ
            for effect_type, examples in result["special_effects"].items():
                all_special_effects[effect_type].update([char_data.get("Name", "")])
                for ex in examples[:1]:  # åªä¿ç•™ç¤ºä¾‹
                    all_special_effects[f"{effect_type}_examples"] = ex
                    
        except Exception as e:
            print(f"Error analyzing {char_id}: {e}")
    
    # æ‰“å°åˆ†æç»“æœ
    print("=" * 70)
    print("æ–‡æ¡£ä¸­å¯èƒ½æœªæ¶µç›–çš„BUFFç±»å‹åˆ†æ")
    print("=" * 70)
    
    effect_categories = {
        "ENERGY_RELATED": "âš¡ èƒ½é‡ç›¸å…³ï¼ˆå……èƒ½ã€å›èƒ½ã€èƒ½é‡æ•ˆç‡ï¼‰",
        "STUN_RELATED": "ğŸ’« å¤±è¡¡ç›¸å…³ï¼ˆå¤±è¡¡å€¼ã€å¤±è¡¡çŠ¶æ€ï¼‰",
        "ANOMALY_RELATED": "ğŸ”® å¼‚å¸¸ç›¸å…³ï¼ˆå±æ€§å¼‚å¸¸ã€å¼‚å¸¸ç§¯è“„/æŒæ§/ç²¾é€šï¼‰",
        "INTERRUPT_IMMUNE": "ğŸ›¡ï¸ æŠ—æ‰“æ–­/æ— æ•Œ/éœ¸ä½“",
        "COMBO_RELATED": "ğŸ”¥ è¿å‡»/è¿æ®µç›¸å…³",
        "SPECIAL_TRIGGER": "ğŸ¯ ç‰¹æ®Šè§¦å‘æ¡ä»¶ï¼ˆæé™é—ªé¿ã€å¿«é€Ÿæ”¯æ´ã€æ‹›æ¶æ”¯æ´ï¼‰",
        "STACK_MECHANIC": "ğŸ“Š å å±‚æœºåˆ¶ï¼ˆå åŠ ã€å±‚æ•°ä¸Šé™ï¼‰",
        "POSITION_BASED": "ğŸ‘¥ ä½ç½®ç›¸å…³ï¼ˆåœºä¸Š/åå°è§’è‰²ï¼‰",
        "ATTRIBUTE_BASED": "ğŸ¨ å±æ€§/é˜µè¥é™å®šï¼ˆç‰¹å®šå±æ€§æˆ–é˜µè¥çš„è§’è‰²ï¼‰",
        "DURATION_BASED": "â±ï¸ æŒç»­æ—¶é—´ç›¸å…³ï¼ˆæŒç»­Xç§’ã€Xç§’å†…ï¼‰",
        "CRIT_TRIGGER": "ğŸ’¥ æš´å‡»è§¦å‘æ•ˆæœï¼ˆæš´å‡»æ—¶å›å¤/å¢ç›Šï¼‰",
    }
    
    for effect_type, desc in effect_categories.items():
        chars = all_special_effects.get(effect_type, set())
        example = all_special_effects.get(f"{effect_type}_examples", "")
        
        print(f"\n{desc}")
        if chars:
            print(f"  æ¶‰åŠè§’è‰²: {', '.join(sorted(chars))}")
        if example:
            print(f"  ç¤ºä¾‹: {example}")
        else:
            print(f"  âš ï¸ æœªæ‰¾åˆ°æ­¤ç±»å‹æ•ˆæœ")
    
    # å»ºè®®è¡¥å……çš„BUFFç±»å‹
    print("\n" + "=" * 70)
    print("ğŸ“‹ å»ºè®®è¡¥å……åˆ°æ–‡æ¡£çš„BUFFç±»å‹")
    print("=" * 70)
    
    recommendations = [
        ("èƒ½é‡æœºåˆ¶", "ENERGY_", "èƒ½é‡è‡ªåŠ¨å›å¤ã€èƒ½é‡è·å¾—æ•ˆç‡ã€å……èƒ½å±‚æ•°ã€èƒ½é‡å›å¤"),
        ("å¤±è¡¡æœºåˆ¶", "STUN_", "å¤±è¡¡å€¼æå‡ã€å¤±è¡¡çŠ¶æ€ä¼¤å®³åŠ æˆã€å¤±è¡¡æŠµæŠ—"),
        ("å¼‚å¸¸æœºåˆ¶", "ANOMALY_", "å¼‚å¸¸ç§¯è“„å€¼ã€å¼‚å¸¸æŒæ§ã€å¼‚å¸¸ç²¾é€šã€å±æ€§å¼‚å¸¸ä¼¤å®³"),
        ("æŠ—æ‰“æ–­", "INTERRUPT_", "æŠ—æ‰“æ–­ç­‰çº§ã€æ— æ•Œæ—¶é—´ã€éœ¸ä½“"),
        ("è¿å‡»åŠ æˆ", "COMBO_", "è¿å‡»ä¼¤å®³åŠ æˆã€è¿å‡»å±æ€§åŠ æˆ"),
        ("ç‰¹æ®Šè§¦å‘", "TRIGGER_", "æé™é—ªé¿è§¦å‘ã€å¿«é€Ÿæ”¯æ´è§¦å‘ã€æ‹›æ¶æ”¯æ´è§¦å‘"),
        ("å å±‚æœºåˆ¶", "STACK_", "æœ€å¤§å±‚æ•°ã€å å±‚è§¦å‘æ¡ä»¶ã€å å±‚æ•ˆæœ"),
        ("ä½ç½®ç›¸å…³", "POSITION_", "åœºä¸Šè§’è‰²å¢ç›Šã€åå°è§’è‰²å¢ç›Š"),
        ("æš´å‡»è§¦å‘", "CRIT_TRIGGER_", "æš´å‡»æ—¶å›å¤/å¢ç›Š/æ•ˆæœ"),
        ("æŒç»­æ—¶é—´", "DURATION_", "å¢ç›ŠæŒç»­æ—¶é—´ã€æ•ˆæœæŒç»­æ—¶é—´"),
    ]
    
    for name, prefix, desc in recommendations:
        print(f"\n{name} ({prefix})")
        print(f"  æè¿°: {desc}")


if __name__ == "__main__":
    main()
